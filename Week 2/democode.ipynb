{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwBCE43Cv3PH"
   },
   "source": [
    "# Gestructureerde data\n",
    "\n",
    "In deze notebook gaan we het klassieke Titanic classificatieprobleem oplossen.\n",
    "In het eerste deel gaan we dit met pytorch doen, in het tweede deel met keras en tensorflow.\n",
    "\n",
    "Hierbij worden de volgende stappen uitgevoerd:\n",
    "- Data inladen\n",
    "- Data Modelling\n",
    "- Model evaluation\n",
    "\n",
    "De dataset die we hierbij gebruiken kan als volgt gedownload worden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
       "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
       "1         1  female  38.0                   1      0  71.2833  First        C   \n",
       "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
       "3         1  female  35.0                   1      0  53.1000  First        C   \n",
       "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
       "\n",
       "   embark_town alone  \n",
       "0  Southampton     n  \n",
       "1    Cherbourg     n  \n",
       "2  Southampton     y  \n",
       "3  Southampton     n  \n",
       "4   Queenstown     y  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Define URL to load the Titanic dataset\n",
    "url = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "df_orig = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df_orig.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze data kan dan als volgt gepreprocessed worden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2687/1785128422.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['age'].fillna(age_mean, inplace=True)\n",
      "/tmp/ipykernel_2687/1785128422.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['embark_town'].fillna(\"unknown\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df = df_orig.copy()\n",
    "\n",
    "# Handle missing data: Fill missing values with the median (for numerical data) or mode (for categorical data)\n",
    "age_mean = df['age'].median()\n",
    "df['age'].fillna(age_mean, inplace=True)\n",
    "df['embark_town'].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "df['sex'] = LabelEncoder().fit_transform(df['sex'])\n",
    "df['class'] = LabelEncoder().fit_transform(df['class'])\n",
    "df['embark_town'] = LabelEncoder().fit_transform(df['embark_town'])\n",
    "df['alone'] = LabelEncoder().fit_transform(df['alone'])\n",
    "df = pd.concat([df, pd.get_dummies(df['deck'], dtype=int)],axis=1)\n",
    "df = df.drop('deck', axis=1)\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(columns=['survived'])\n",
    "y = df['survived']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling met pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.6599463410675526, Validation Loss: 0.637076199054718\n",
      "Epoch 2, Training Loss: 0.638684269040823, Validation Loss: 0.614509791135788\n",
      "Epoch 3, Training Loss: 0.6167835108935833, Validation Loss: 0.5869619250297546\n",
      "Epoch 4, Training Loss: 0.5899760611355305, Validation Loss: 0.5546158701181412\n",
      "Epoch 5, Training Loss: 0.5591618604958057, Validation Loss: 0.5167573317885399\n",
      "Epoch 6, Training Loss: 0.5247539039701223, Validation Loss: 0.4839297905564308\n",
      "Epoch 7, Training Loss: 0.49535467103123665, Validation Loss: 0.4559408128261566\n",
      "Epoch 8, Training Loss: 0.46660900861024857, Validation Loss: 0.4400290176272392\n",
      "Epoch 9, Training Loss: 0.44957354851067066, Validation Loss: 0.4291984736919403\n",
      "Epoch 10, Training Loss: 0.43465789780020714, Validation Loss: 0.424580417573452\n",
      "Epoch 11, Training Loss: 0.4217523820698261, Validation Loss: 0.4235852286219597\n",
      "Epoch 12, Training Loss: 0.41702260076999664, Validation Loss: 0.4209333658218384\n",
      "Epoch 13, Training Loss: 0.4114722181111574, Validation Loss: 0.41889840364456177\n",
      "Epoch 14, Training Loss: 0.4133403245359659, Validation Loss: 0.4196382239460945\n",
      "Epoch 15, Training Loss: 0.40527000557631254, Validation Loss: 0.4198639243841171\n",
      "Epoch 16, Training Loss: 0.40460278559476137, Validation Loss: 0.4223211519420147\n",
      "Epoch 17, Training Loss: 0.39845868945121765, Validation Loss: 0.4196968153119087\n",
      "Epoch 18, Training Loss: 0.4012102223932743, Validation Loss: 0.421216756105423\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the Feed-Forward Neural Network model in PyTorch\n",
    "class TitanicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TitanicModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(16, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, define the loss function and optimizer\n",
    "model = TitanicModel()\n",
    "criterion = nn.BCELoss() # deze verwacht een sigmoid activatiefunctie\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model with Early Stopping\n",
    "early_stopping_tolerance = 5\n",
    "min_loss = np.inf\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss/len(train_loader)}, Validation Loss: {val_loss/len(test_loader)}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= early_stopping_tolerance:\n",
    "            print(\"Early stopping\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder staat de code om een visuele weergave van het model te bekomen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_structure.png'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Gebruik een voorbeeldinput om het computationele grafiek te genereren\n",
    "example_input = torch.randn(1, 16)  # A batch of 1, with 7 features (assuming 7 input features)\n",
    "output = model(example_input)\n",
    "\n",
    "# Maak een visualisatie van het computationele grafiek\n",
    "make_dot(output, params=dict(model.named_parameters())).render(\"model_structure\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 32]             544\n",
      "              ReLU-2                   [-1, 32]               0\n",
      "            Linear-3                   [-1, 16]             528\n",
      "              ReLU-4                   [-1, 16]               0\n",
      "            Linear-5                    [-1, 1]              17\n",
      "           Sigmoid-6                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 1,089\n",
      "Trainable params: 1,089\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(16,), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Define the Feed-Forward Neural Network model in TensorFlow\n",
    "model_tf = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(16,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_tf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks for Early Stopping and TensorBoard\n",
    "early_stopping_cb = callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir='./logs')\n",
    "\n",
    "# Train the model with Early Stopping and TensorBoard callbacks\n",
    "history = model_tf.fit(X_train, y_train, validation_split=0.2, epochs=100, \n",
    "                       callbacks=[early_stopping_cb, tensorboard_cb], batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model_tf.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Create predictions for the confusion matrix\n",
    "y_pred_tf = (model_tf.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_tf = confusion_matrix(y_test, y_pred_tf)\n",
    "disp_tf = ConfusionMatrixDisplay(confusion_matrix=cm_tf)\n",
    "disp_tf.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teken de graaf van dit preprocessing model\n",
    "tf.keras.utils.plot_model(model = model_tf , rankdir=\"LR\", dpi=72, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print de samenvatting van dit preprocessing model\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow met preprocessing in het netwerk\n",
    "\n",
    "Hierboven werd het preprocessing gedeelte nog gedaan met sci-kit learn.\n",
    "Het kan beter zijn voor productie om de (sommige) preprocessing mee te betrekken in het neuraal netwerk.\n",
    "Dit om te voorkomen dat je alle preprocessingstappen moet beschikbaar maken voor developers die het op dezelfde manier moeten implementeren.\n",
    "Hierdoor kan je soms data vrijgeven (zoals gemiddelden voor scalers, woordenboeken voor categorieke data, ...) wat niet gewenst is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Normalization, StringLookup, CategoryEncoding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "# Split into features and labels\n",
    "target = \"survived\"\n",
    "features = df_orig.copy()\n",
    "labels = features.pop(target)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_columns = ['sex', 'class', 'deck', 'embark_town', 'alone']\n",
    "numerical_columns = ['age', 'n_siblings_spouses', 'parch', 'fare']\n",
    "\n",
    "\n",
    "# Create Input layers and preprocessing layers\n",
    "inputs = {}\n",
    "preprocessed_inputs = []\n",
    "\n",
    "# Process numerical features\n",
    "for col in numerical_columns:\n",
    "    input_col = Input(shape=(1,), name=col)\n",
    "    # Create a Normalization layer and adapt it to the data\n",
    "    normalization_layer = Normalization()\n",
    "    normalization_layer.adapt(features[col].values.reshape(-1, 1))\n",
    "    normalized_col = normalization_layer(input_col)\n",
    "    inputs[col] = input_col\n",
    "    preprocessed_inputs.append(normalized_col)\n",
    "\n",
    "# Process categorical features\n",
    "for col in categorical_columns:\n",
    "    input_col = Input(shape=(1,), name=col, dtype=tf.string)\n",
    "    # Create a StringLookup layer and adapt it to the data\n",
    "    lookup_layer = StringLookup(output_mode='int')  # output_mode='int' for one-hot encoding\n",
    "    lookup_layer.adapt(features[col])\n",
    "    encoded_col = lookup_layer(input_col)\n",
    "    # One-hot encode the integer categorical indices\n",
    "    one_hot_layer = CategoryEncoding(num_tokens=lookup_layer.vocabulary_size(), output_mode='one_hot')\n",
    "    one_hot_col = one_hot_layer(encoded_col)\n",
    "    inputs[col] = input_col\n",
    "    preprocessed_inputs.append(one_hot_col)\n",
    "\n",
    "# Concatenate all preprocessed inputs\n",
    "all_features = Concatenate()(preprocessed_inputs)\n",
    "\n",
    "# Define the neural network model\n",
    "x = Dense(128, activation='relu')(all_features)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output = Dense(1, activation='sigmoid')(x)  # Output layer for binary classification\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Convert the DataFrame to a dictionary of NumPy arrays\n",
    "features_dict = {name: tf.convert_to_tensor(features[name].values) for name in features.columns}\n",
    "\n",
    "# Train the model\n",
    "model.fit(x=features_dict, y=labels, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teken de graaf van dit preprocessing model\n",
    "\n",
    "tf.keras.utils.plot_model(model = model , rankdir=\"LR\", dpi=72, show_shapes=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pandas_dataframe.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
