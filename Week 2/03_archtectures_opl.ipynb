{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwBCE43Cv3PH"
   },
   "source": [
    "# Gestructureerde data: Complexere architecturen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de voorgaande onderdelen heb je reeds gemerkt dat het verwerken van de input in het geval van gestructureerde data kan leiden tot een complexere netwerkarchitectuur.\n",
    "In deze notebook gaan we dit in meer detail bestuderen en ook kijken naar de mogelijkheden aan de output-kant van het netwerk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputkant\n",
    "\n",
    "Voor dit deel verwijs ik door naar de vorige notebook over het preprocessen van de input. Vooral in het geval van pytorch is er niet veel mogelijkheden om dit in pytorch zelf te manipuleren.\n",
    "\n",
    "In tensorflow is het belangrijk stil te staan bij het feit dat er twee mogelijkheden zijn om een model te maken.\n",
    "* Sequential API\n",
    "* Functional API\n",
    "\n",
    "De sequential API verwacht dat alle data door alle lagen gestuurd wordt. Hierdoor is differentiatie in de input niet mogelijk. De functional API daarentegen is flexibeler en maakt deze differentiatie wel mogelijk door met verschillende input-tensors te werken die elk op hun eigen manier verwerkt kunnen worden.\n",
    "\n",
    "Een voorbeeld van beide methodes staat hieronder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "\n",
    "# Preprocess data: handle missing values and convert categorical data\n",
    "titanic.fillna({'age': titanic['age'].median(), 'embark_town': titanic['embark_town'].mode()[0]}, inplace=True)\n",
    "titanic['sex'] = titanic['sex'].astype('category').cat.codes\n",
    "titanic['class'] = titanic['class'].astype('category').cat.codes\n",
    "titanic['embark_town'] = titanic['embark_town'].astype('category').cat.codes\n",
    "\n",
    "# Custom PyTorch Dataset\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        # Separate numerical and categorical features\n",
    "        self.num_features = dataframe[['age', 'n_siblings_spouses', 'parch', 'fare']]\n",
    "        self.cat_features = dataframe[['class', 'sex', 'embark_town']]\n",
    "        self.target = dataframe['survived']\n",
    "\n",
    "        # Convert to tensors\n",
    "        self.num_features = torch.tensor(self.num_features.values, dtype=torch.float32)\n",
    "        self.cat_features = torch.tensor(self.cat_features.values, dtype=torch.float32)\n",
    "        self.target = torch.tensor(self.target.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.num_features[idx], self.cat_features[idx], self.target[idx]\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TitanicDataset(titanic)\n",
    "\n",
    "# Split dataset into training and testing datasets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataset.num_features)\n",
    "\n",
    "# Create PyTorch model with multiple inputs\n",
    "class ComplexModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_categories):\n",
    "        super(ComplexModel, self).__init__()\n",
    "        # Layers for numerical features\n",
    "        self.num_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_features, 32),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        # Layers for categorical features\n",
    "        self.cat_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_categories, 10),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.shared_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(42, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, num_input, cat_input):\n",
    "        num_out = self.num_layers(num_input)\n",
    "        cat_out = self.cat_layers(cat_input)\n",
    "        combined = torch.cat((num_out, cat_out), dim=1)\n",
    "        return self.shared_layers(combined)\n",
    "\n",
    "# Define the model\n",
    "model = ComplexModel(num_features=dataset.num_features.shape[1], num_categories=len(dataset.cat_features.unique(dim=0)[0]))\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (num_data, cat_data, target) in enumerate(train_loader):\n",
    "        # Standardize numerical features\n",
    "        num_data = torch.tensor(scaler.transform(num_data.numpy()))\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(num_data, cat_data)\n",
    "        loss = criterion(output, target.unsqueeze(-1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import Input\n",
    "from keras import layers\n",
    "\n",
    "titanic_features = titanic.copy()\n",
    "titanic_labels = titanic_features.pop('survived')\n",
    "\n",
    "inputs = {}\n",
    "\n",
    "for name, column in titanic_features.items():\n",
    "  dtype = column.dtype\n",
    "  if dtype == object:\n",
    "    dtype = tf.string\n",
    "  else:\n",
    "    dtype = tf.float32\n",
    "\n",
    "  inputs[name] = Input(shape=(1,), name=name, dtype=dtype)\n",
    "\n",
    "# selecteer gelijkaardige inputs\n",
    "numeric_inputs = {name:input for name,input in inputs.items()\n",
    "                  if input.dtype==tf.float32}\n",
    "categoric_inputs = {name:input for name,input in inputs.items()\n",
    "                  if input.dtype==tf.string}\n",
    "\n",
    "# om alle verschillende eindresultaten bij te houden\n",
    "preprocessed_inputs = []\n",
    "\n",
    "# preprocess numeriek\n",
    "x = layers.Concatenate()(list(numeric_inputs.values()))\n",
    "norm = layers.Normalization()\n",
    "norm.adapt(np.array(titanic[numeric_inputs.keys()]))\n",
    "\n",
    "preprocessed_inputs.append(norm(x))\n",
    "\n",
    "# preprocess categoriek\n",
    "for name, input in categoric_inputs.items():\n",
    "  lookup = layers.StringLookup(vocabulary=np.unique(titanic_features[name]))\n",
    "  one_hot = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n",
    "\n",
    "  x = lookup(input)\n",
    "  x = one_hot(x)\n",
    "    \n",
    "  preprocessed_inputs.append(x)\n",
    "\n",
    "# voeg alle verwerkte inputs samen\n",
    "preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n",
    "\n",
    "x = layers.Dense(64, activation='relu')(preprocessed_inputs_cat)\n",
    "output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic_features, titanic_labels, test_size=0.2, random_state=42)\n",
    "titanic_features.info()\n",
    "\n",
    "# Convert pandas DataFrames to TensorFlow Datasets\n",
    "X_train_dict = {name: np.array(value) for name, value in X_train.items()}\n",
    "X_test_dict = {name: np.array(value) for name, value in X_test.items()}\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_dict, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meerdere outputs/acties door 1 model\n",
    "\n",
    "In sommige gevallen kan het nodig zijn dat je simultaan meerdere voorspellingen doet voor dezelfde data.\n",
    "In plaats van twee modellen te trainen is het veel efficienter om hetzelfde model te gebruiken maar met meerdere outputs. \n",
    "In het voorbeeld hieronder kan je zien hoe je dit moet doen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV data into a pandas DataFrame\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Convert DataFrame to NumPy arrays\n",
    "features = data[['age', 'weight', 'height']].to_numpy()\n",
    "targets = data[['score1', 'score2']].to_numpy()\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "features = torch.from_numpy(features).float()\n",
    "targets = torch.from_numpy(targets).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiOutputRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 16)  # 3 input features, 16 hidden units\n",
    "        self.fc2 = nn.Linear(16, 32)  # 16 hidden units, 32 hidden units\n",
    "        self.fc3 = nn.Linear(32, 2)  # 32 hidden units, 2 output units (regression scores)  --> 2 outputs (regression)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "model = MultiOutputRegressor()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model for a specified number of epochs\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    outputs = model(features)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass and update weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}: Loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "features = tf.convert_to_tensor(features, dtype=tf.float32)\n",
    "targets = tf.convert_to_tensor(targets, dtype=tf.float32)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=2, input_shape=[3])\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(features, targets, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verschillende loss functies**\n",
    "\n",
    "Voorgaande voorbeelden gebruikten dezelfde loss-functie voor de verschillende outputs. Dit omdat de voorspellingen identiek waren (beide outputs regressie).\n",
    "Het is ook mogelijk om verschillende loss-functies te gebruiken voor elke output of voor verschillende subsets van outputs.\n",
    "\n",
    "Onderstaande voorbeeld toont hoe dit eruit kan zien door middel van een regressieprobleem en een classificatie probleem met drie klassen te combineren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV data into a pandas DataFrame\n",
    "data = pd.read_csv('data2.csv')\n",
    "\n",
    "# Convert DataFrame to NumPy arrays\n",
    "features = data[['age', 'weight', 'height']].to_numpy()\n",
    "targets_regression = data[['score']].to_numpy()\n",
    "targets_cls = data['class'].astype('category').cat.codes.to_numpy() # zet a,b,c van dataset om naar 0,1,2\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "features = torch.from_numpy(features).float()\n",
    "targets_regression = torch.from_numpy(targets_regression).float()\n",
    "targets_cls = torch.from_numpy(targets_cls).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 16)  # 3 input features, 16 hidden units\n",
    "        self.fc2 = nn.Linear(16, 32)  # 16 hidden units, 32 hidden units\n",
    "        self.fc3_reg = nn.Linear(32, 1)  # 32 hidden units, 1 regression outputs\n",
    "        self.fc3_cls = nn.Linear(32, num_classes)  # 32 hidden units, num_classes classification outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        reg_output = self.fc3_reg(x)\n",
    "        cls_output = self.fc3_cls(x)\n",
    "        return reg_output, cls_output\n",
    "\n",
    "def combined_loss(reg_pred, cls_pred, reg_target, cls_target):\n",
    "    reg_loss = F.mse_loss(reg_pred, reg_target)\n",
    "    cls_loss = F.cross_entropy(cls_pred, cls_target)\n",
    "    return reg_loss + cls_loss\n",
    "\n",
    "model = CombinedModel(num_classes=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    reg_pred, cls_pred = model(features)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = combined_loss(reg_pred, cls_pred, targets_regression, targets_cls)\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass and update weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}: Loss = {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "num_classes=3\n",
    "inputs = Input(shape=(3,))\n",
    "x = layers.Dense(32, activation='relu')(inputs)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# Regression output\n",
    "reg_output = layers.Dense(1)(x)\n",
    "\n",
    "# Classification output\n",
    "cls_output = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[reg_output, cls_output])\n",
    "\n",
    "# Custom loss function combining MSE for regression and categorical crossentropy for classification\n",
    "def custom_loss(y_true, y_pred):\n",
    "    reg_loss = tf.keras.losses.mse(y_true[0], y_pred[0])\n",
    "    cls_loss = tf.keras.losses.categorical_crossentropy(y_true[1], y_pred[1])\n",
    "    return reg_loss + cls_loss\n",
    "\n",
    "model.compile(loss=custom_loss, optimizer='adam')\n",
    "\n",
    "# Convert y_cls to one-hot encoding\n",
    "y_cls_one_hot = tf.one_hot(targets_cls, num_classes)\n",
    "\n",
    "# Combine regression and classification targets\n",
    "y = [targets_regression, y_cls_one_hot]\n",
    "\n",
    "model.fit(features, y, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pandas_dataframe.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
