{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwBCE43Cv3PH"
   },
   "source": [
    "# Gestructureerde data: inladen dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQB7yiF6v9GR"
   },
   "source": [
    "## Enkel datatype\n",
    "\n",
    "Er zijn heel veel verschillende bronnen om datasets te vinden. Je kan ze bijvoorbeeld op keras, overheidssites en dergelijke vinden.\n",
    "Ook zijn er heel wat datasets die standaard geworden zijn voor voorbeelden/modellen te testen. \n",
    "Deze datasets worden mee aangeleverd met de verschillende modelling-frameworks en kunnen zo eenvoudig gebruikt worden zonder een extra download uit te voeren.\n",
    "In deze notebook gaan we werken met deze standaard datasets.\n",
    "\n",
    "In het eerste voorbeeld gaan we werken met een dataset waarbij alle data reeds numeriek is, meer bepaald gaan we werken met de iris-dataset dat informatie over de bloemen van een aantal iris-soorten bevat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch\n",
    "\n",
    "Om een dataset in lezen en klaar te maken in pytorch moet je werken met een Dataset. Dit kan je doen als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define custom PyTorch Dataset\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras / Tensorflow\n",
    "\n",
    "Keras is vooral bedoeld als uitvoerend framework en niet om data in te laden. Hierdoor moeten we dus eerder werken met een tensorflow structuur om data in te laden.\n",
    "In het geval van Tesnsorflow zijn er een aantal manieren om te werken met gestructureerde data.\n",
    "De belangrijkste zijn in het geval van gestructureerde data zijn:\n",
    "* Rechstreeks werken met numpy array/dataframes X en y (niet hier getoond want geen extra code nodig)\n",
    "* from_tensor_slices: de dataset reeds ingelezen in dataframe/numpy array en gepreprocessed in een ander framework\n",
    "* make_csv_dataset: Lees rechstreeks een csv in (belangrijk om het aantal epochs hier ook in te stellen (anders loopt het continue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal.length        : [5.5 4.8 6.1 7.3 7. ]\n",
      "sepal.width         : [4.2 3.  2.6 2.9 3.2]\n",
      "petal.length        : [1.4 1.4 5.6 6.3 4.7]\n",
      "petal.width         : [0.2 0.3 1.4 1.8 1.4]\n",
      "\n",
      "label               : [b'Setosa' b'Setosa' b'Virginica' b'Virginica' b'Versicolor']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 08:50:06.940090: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.data import Dataset\n",
    "from tensorflow.data.experimental import make_csv_dataset\n",
    "\n",
    "# methode: from_tensor_slices\n",
    "train_ds = Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "test_ds = Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
    "\n",
    "# methode: lees csv\n",
    "iris_ds = make_csv_dataset(\n",
    "    'iris.csv',\n",
    "    batch_size=5, # Klein om voorbeelden eenvoudiger te maken\n",
    "    label_name='variety',\n",
    "    num_epochs=1) # Voeg dit zeker toe\n",
    "\n",
    "for batch, label in iris_ds.take(1):\n",
    "  for key, value in batch.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "  print()\n",
    "  print(f\"{'label':20s}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed types\n",
    "\n",
    "Bovenstaande voorbeelden zijn eenvoudige voorbeelden omdat de data in principe reeds gepreprocessed is of uit slechts 1 datatype bestaat. \n",
    "Dit heeft als gevolg dat alle inputs op dezelfde manier verwerkt kunnen worden en er dus geen onderscheid gemaakt moet worden tussen verschillende kolommen.\n",
    "Data met hetzelfde type kan dus eenvoudig aan een sequentieel neuraal netwerk model gepresenteerd worden.\n",
    "\n",
    "Indien de dataset echter meerdere types bevatten kan de data niet rechtstreeks aan het neuraal netwerk doorgegeven worden aangezien dit type model enkel met numerieke data kan werken. \n",
    "Een standaard dataset waarin dit gebeurd is de titanic dataset.\n",
    "Hoe dit gebeurd in de verschillende frameworks zie je hieronder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch\n",
    "\n",
    "Aangezien we data binnenkrijgen in een dataset als numpy array/dataframe kunnen we in de constructor of de get_item functie de nodige preprocessing stappen uitvoeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "\n",
    "# Define custom PyTorch Dataset\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        # Preprocess data: handle missing values and convert categorical data\n",
    "        dataframe.fillna({'age': dataframe['age'].median()}, inplace=True)\n",
    "        dataframe.fillna({'embark_town': dataframe['embark_town'].mode()[0]}, inplace=True)\n",
    "        dataframe['sex'] = dataframe['sex'].astype('category').cat.codes\n",
    "        dataframe['class'] = dataframe['class'].astype('category').cat.codes\n",
    "        dataframe['embark_town'] = dataframe['embark_town'].astype('category').cat.codes\n",
    "        \n",
    "        # Select relevant columns\n",
    "        X = dataframe[['class', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare', 'embark_town']]\n",
    "        y = dataframe['survived']\n",
    "        \n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TitanicDataset(titanic)\n",
    "\n",
    "# Split dataset into training and testing datasets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras/Tensorflow\n",
    "\n",
    "Dit stukje is de meest complexe vorm om data in te laden maar misschien ook het meest flexible.\n",
    "We gaan namelijk van een dataset met mixed types een sequentieel model maken waarbij de nodige preprocessing stappen aanwezig zijn in het model.\n",
    "Dit gaat uiteindelijk leiden tot een vrij complexe modelopbouw.\n",
    "\n",
    "Omdat preprocessing functionaliteiten slechts in de volgende stap gaan bestudeerd worden gaan we hier enkel kijken hoe we de data kunnen klaar zetten.\n",
    "De twee methodes hiervoor zijn:\n",
    "* Plaats elke input/feature in een apart Input object. Dit zijn symbolische tensors die gebruikt worden als placeholder voor onze data\n",
    "* Maak een csv dataset (identiek als hierboven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sex': <KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=sex>,\n",
       " 'age': <KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=age>,\n",
       " 'n_siblings_spouses': <KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=n_siblings_spouses>,\n",
       " 'parch': <KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=parch>,\n",
       " 'fare': <KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=fare>,\n",
       " 'class': <KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=class>,\n",
       " 'deck': <KerasTensor shape=(None, 1), dtype=string, sparse=None, name=deck>,\n",
       " 'embark_town': <KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=embark_town>,\n",
       " 'alone': <KerasTensor shape=(None, 1), dtype=string, sparse=None, name=alone>}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Input\n",
    "\n",
    "titanic_features = titanic.copy()\n",
    "titanic_labels = titanic_features.pop('survived')\n",
    "\n",
    "inputs = {}\n",
    "\n",
    "for name, column in titanic_features.items():\n",
    "  dtype = column.dtype\n",
    "  if dtype == object:\n",
    "    dtype = tf.string\n",
    "  else:\n",
    "    dtype = tf.float32\n",
    "\n",
    "  inputs[name] = Input(shape=(1,), name=name, dtype=dtype)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de numerieke inputs kunnen dan als volgt geselecteerd worden\n",
    "numeric_inputs = {name:input for name,input in inputs.items()\n",
    "                  if input.dtype==tf.float32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'female' b'male' b'male' b'female']\n",
      "age                 : [36. 24. 29. 50. 29.]\n",
      "n_siblings_spouses  : [0 0 1 1 0]\n",
      "parch               : [0 0 0 0 0]\n",
      "fare                : [  0.      69.3      7.0458 106.425  211.3375]\n",
      "class               : [b'Third' b'First' b'Third' b'First' b'First']\n",
      "deck                : [b'unknown' b'B' b'unknown' b'C' b'B']\n",
      "embark_town         : [b'Southampton' b'Cherbourg' b'Southampton' b'Cherbourg' b'Southampton']\n",
      "alone               : [b'y' b'y' b'n' b'n' b'y']\n",
      "\n",
      "label               : [0 1 0 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 09:26:23.438416: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "titanic_file_path = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "titanic_ds = tf.data.experimental.make_csv_dataset(\n",
    "    titanic_file_path,\n",
    "    batch_size=5, \n",
    "    label_name='survived',\n",
    "    num_epochs=1)\n",
    "\n",
    "for batch, label in titanic_ds.take(1):\n",
    "  for key, value in batch.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "  print()\n",
    "  print(f\"{'label':20s}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pandas_dataframe.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
