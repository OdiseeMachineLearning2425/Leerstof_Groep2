{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac16090",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "Het verzamelen van voldoende data kan zeer tijdrovend en kostelijk zijn.\n",
    "Daarom zou het handig zijn om op een automatische manier extra data aan te maken.\n",
    "Dit is wat data augmentation doet en wordt vaak gebruikt wanneer er gewerkt wordt met visuele of auditieve data.\n",
    "De code hieronder zal vooral een voorbeeld zijn van hoe je data augmentatie kan toepassen op beelden/images.\n",
    "Hierbij gaan we voor het trainen op een random manier een variatie maken van de figuur.\n",
    "Doordat er meerdere epochs gebruikt worden bij training zullen er op die manier meerdere varianten van elke figuur gebruikt worden bij training.\n",
    "\n",
    "Data augmentation kan je meestal toepassen zonder problemen. Zeker de opties waarbij geen pixels verdwijnen geven normaal geen issues.\n",
    "Let echter wel op als je roteert of cropt en hetgene dat je moet detecteren aan de rand van je scherm ligt, dat dat object kan verdwijnen.\n",
    "Indien je genoeg epochs gebruikt zou dit echter geen groot probleem mogen zijn door compensatie in de andere epochs.\n",
    "\n",
    "In de vorige notebook hadden we onderstaande model om een classificatie CNN te maken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20194191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Stap 1: Dataset Inladen en Voorbereiden\n",
    "transform = transforms.Compose([ \n",
    "    transforms.ToTensor(),  # Zet de afbeelding om naar een tensor en normaliseer naar [0, 1]\n",
    "])\n",
    "\n",
    "# Laad de CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "# Define the CNN model using torch.nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 6, 5),       # Convolutional layer: input channels=3, output channels=6, kernel size=5\n",
    "    nn.ReLU(),                # ReLU activation\n",
    "    nn.MaxPool2d(2, 2),       # Max pooling layer: kernel size=2, stride=2\n",
    "    nn.Conv2d(6, 16, 5),      # Convolutional layer: input channels=6, output channels=16, kernel size=5\n",
    "    nn.ReLU(),                # ReLU activation\n",
    "    nn.MaxPool2d(2, 2),       # Max pooling layer: kernel size=2, stride=2\n",
    "    nn.Flatten(),             # Flatten the output for the fully connected layers\n",
    "    nn.Linear(16 * 5 * 5, 120), # Fully connected layer: input features=16*5*5, output features=120\n",
    "    nn.ReLU(),                # ReLU activation\n",
    "    nn.Linear(120, 84),       # Fully connected layer: input features=120, output features=84\n",
    "    nn.ReLU(),                # ReLU activation\n",
    "    nn.Linear(84, 10)         # Output layer: input features=84, output features=10 (number of classes)\n",
    ")\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(2):  # Number of epochs\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # Print every 2000 batches\n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate the model\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Make predictions\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Print the image, true labels, and predicted labels\n",
    "def imshow(img):\n",
    "    img = img.permute(1, 2, 0).numpy()  # Convert back to HxWxC\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "# Print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{trainset.classes[labels[j]]:5s}' for j in range(4)))\n",
    "print('Predicted:  ', ' '.join(f'{trainset.classes[predicted[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61a0a9-91af-4f12-8013-8c5667a8d816",
   "metadata": {},
   "source": [
    "Om op de cifar10 dataset data augmentation uit te voeren moeten we enkel de transform stappenplan aanpassen om op een random manier bewerkingen uit te voeren op de figuren.\n",
    "We kunnen bijvoorbeeld de transform aanpassen als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf52cb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a7db909-c56a-40d0-b251-87b26c4c479c",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "De analoge oplossing om dit te doen met keras is door het model in onderstaande voorbeeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc4ca7-ea3d-4010-a6e8-8d2dbb745f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Rescaling\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, Rescaling, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Laad en bereid de CIFAR-10 dataset voor\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Min-Max scaling naar [0, 1]\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Definieer het CNN-model met tf.keras.Sequential\n",
    "model = Sequential([\n",
    "    Rescaling(1.0/255.0),\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile het model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train het model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=64,\n",
    "                    validation_split=0.1, verbose=2)\n",
    "\n",
    "# Evalueer het model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f'\\nTest accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Maak voorspellingen\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5815101d-25da-4c2b-a608-853f5c2d03d1",
   "metadata": {},
   "source": [
    "aan te passen op twee mogelijke manieren. \n",
    "Ten eerste kunnen de random transformaties uitgevoerd worden door een ImageDataGenerator zoals in onderstaande voorbeeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1de32d-6e75-4a12-8c65-d87a2f54cb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ae1d2a7-3fb4-49e4-9458-4bd7a43fdfff",
   "metadata": {},
   "source": [
    "of door extra lagen toe te voegen aan het model als volgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f008e2b-1535-4a16-af3d-98b141b387d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c4a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
