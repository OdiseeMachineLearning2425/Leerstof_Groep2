{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4546c4a-95ce-4bf5-b09e-fd32d2d4c954",
   "metadata": {},
   "source": [
    "# Image segmentation\n",
    "\n",
    "In dit voorbeeld zullen we gebruik maken van de Oxford Pets Dataset. Het model zal leren om objecten (bijv. huisdieren) van de achtergrond te onderscheiden.\n",
    "We gebruiken hiervoor de U-Net architectuur, een populair model voor afbeeldingssegmentatie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d54e0a4a-0733-4efd-bd35-d46a8e697488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: oxford/images.tar.gz\n",
      "Using downloaded and verified file: oxford/annotations.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import opendatasets as od\n",
    "import tarfile\n",
    "\n",
    "od.download(\"http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\", data_dir=\"oxford\")\n",
    "od.download(\"http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\", data_dir=\"oxford\")\n",
    "\n",
    "# Controleren of GPU beschikbaar is\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Open het .tar.gz bestand\n",
    "with tarfile.open(\"oxford/images.tar.gz\", \"r:gz\") as tar:\n",
    "    # Pak alle inhoud uit naar de \"oxford/images\" directory\n",
    "   # tar.extractall(path=\"oxford\")\n",
    "    pass\n",
    "\n",
    "# Open het .tar.gz bestand\n",
    "with tarfile.open(\"oxford/annotations.tar.gz\", \"r:gz\") as tar:\n",
    "    # Pak alle inhoud uit naar de \"oxford/images\" directory\n",
    "   # tar.extractall(path=\"oxford\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e53b5d-14c4-406b-8269-4e8dc5b465d9",
   "metadata": {},
   "source": [
    "## Dataset inladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc756d13-861e-4e06-8c9f-8229f1dad38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oxford/images/Siamese_25.jpg\n",
      "oxford/images/samoyed_66.jpg\n",
      "oxford/images/staffordshire_bull_terrier_76.jpg\n",
      "oxford/images/keeshond_69.jpg\n",
      "oxford/images/Abyssinian_134.jpg\n",
      "oxford/images/english_cocker_spaniel_82.jpg\n",
      "oxford/images/american_bulldog_50.jpg\n",
      "oxford/images/Russian_Blue_55.jpg\n",
      "torch.Size([8, 3, 572, 572])\n",
      "torch.Size([8, 388, 388])\n",
      "oxford/images/Persian_128.jpg\n",
      "oxford/images/pomeranian_34.jpg\n",
      "oxford/images/great_pyrenees_79.jpg\n",
      "oxford/images/samoyed_133.jpg\n",
      "oxford/images/Siamese_207.jpg\n",
      "oxford/images/german_shorthaired_140.jpg\n",
      "oxford/images/Russian_Blue_164.jpg\n",
      "oxford/images/miniature_pinscher_127.jpg\n",
      "torch.Size([8, 3, 572, 572])\n",
      "torch.Size([8, 388, 388])\n",
      "oxford/images/miniature_pinscher_112.jpg\n",
      "oxford/images/leonberger_70.jpg\n",
      "oxford/images/Sphynx_244.jpg\n",
      "oxford/images/Abyssinian_5.jpg\n",
      "oxford/images/beagle_168.jpg\n",
      "oxford/images/leonberger_147.jpg\n",
      "oxford/images/american_pit_bull_terrier_13.jpg\n",
      "oxford/images/Bombay_25.jpg\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 572, 572] at entry 0 and [4, 572, 572] at entry 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m dataset \u001b[38;5;241m=\u001b[39m OxfordPetDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moxford\u001b[39m\u001b[38;5;124m'\u001b[39m, transform)\n\u001b[1;32m     42\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, mask \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(mask\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 572, 572] at entry 0 and [4, 572, 572] at entry 3"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((572, 572)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class OxfordPetDataset(Dataset):\n",
    "    def __init__(self, root, transform = None, output_size=388):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.images = sorted([f for f in os.listdir(os.path.join(self.root, 'images')) if f.endswith('.jpg')])\n",
    "        self.masks = sorted([f for f in os.listdir(os.path.join(root, 'annotations/trimaps')) if f.endswith('.png') and not f.startswith(\".\")])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image\n",
    "        img_path = os.path.join(self.root, 'images', self.images[idx])\n",
    "        print(img_path)\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # mask\n",
    "        mask_path = os.path.join(self.root, 'annotations/trimaps', self.masks[idx])\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        mask = transforms.PILToTensor()(mask)\n",
    "        mask = transforms.Resize((self.output_size, self.output_size))(mask)\n",
    "\n",
    "         # Segmentatiemaskers zijn geannoteerd met waarden 1, 2, 3\n",
    "        mask = mask.squeeze(0)  # Verwijder kanaal 0, aangezien het grijswaarden zijn\n",
    "        # klasse 1  = achtergrond 2 = rand = 3 huisdier -> zet het om naar 0 = achtergrond 1 = huisdier\n",
    "        mask = torch.where(mask >= 2.0, torch.ones_like(mask, dtype=torch.float), torch.zeros_like(mask, dtype=torch.float))  # Mwaarde 2 omzetten naar 1, andere 0)  \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "dataset = OxfordPetDataset('oxford', transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "for image, mask in dataloader:\n",
    "    print(image.shape)\n",
    "    print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a70bd-ddea-40eb-ad91-5dedfcb451ca",
   "metadata": {},
   "source": [
    "## Samenstellen van het U-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7484d73-5bba-404f-9b51-a98c56e7ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__() # constructor base-class\n",
    "\n",
    "        # encoder\n",
    "        self.enc_block1 = self.conv_block(3, 64)\n",
    "        self.enc_block2 = self.conv_block(64, 128)\n",
    "        self.enc_block3 = self.conv_block(128, 256)\n",
    "        self.enc_block4 = self.conv_block(256, 512)\n",
    "\n",
    "        # bottleneck\n",
    "        self.bottleneck = self.conv_block(512, 1024)\n",
    "\n",
    "        # decoder\n",
    "        self.dec_block1 = self.conv_block(1024 + 512, 512) # bottleneck + enc_block4\n",
    "        self.dec_block2 = self.conv_block(512+256, 256) # dec_block1 + enc_block3\n",
    "        self.dec_block3 = self.conv_block(256+128, 128)\n",
    "        self.dec_block4 = self.conv_block(128+64, 64)\n",
    "\n",
    "        # de 1 hier bepaalt hoeveel klassen we toelaten. In dit geval 1 want binaire classificatie (huisdier of niet)\n",
    "        # als er meer opties zijn: is het 1 per klasse (en eventueel 1 extra voor background)\n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        enc_block1 = self.enc_block1(x)\n",
    "        #print(enc_block1.shape)\n",
    "        enc_block2 = self.enc_block2(self.maxpool(enc_block1))\n",
    "        #print(enc_block2.shape)\n",
    "        enc_block3 = self.enc_block3(self.maxpool(enc_block2))\n",
    "        #print(enc_block3.shape)\n",
    "        enc_block4 = self.enc_block4(self.maxpool(enc_block3))\n",
    "        #print(enc_block4.shape)\n",
    "\n",
    "        # bottleneck\n",
    "        bottleneck = self.bottleneck(self.maxpool(enc_block4))\n",
    "\n",
    "        # decoder\n",
    "        dec_block1 = self.upsample_and_concat(bottleneck, enc_block4) # voeg bottleneck en enc_block4\n",
    "        dec_block1 = self.dec_block1(dec_block1)\n",
    "        \n",
    "        dec_block2 = self.upsample_and_concat(dec_block1, enc_block3) # voeg bottleneck en enc_block4\n",
    "        dec_block2 = self.dec_block2(dec_block2)\n",
    "        \n",
    "        dec_block3 = self.upsample_and_concat(dec_block2, enc_block2) # voeg bottleneck en enc_block4\n",
    "        dec_block3 = self.dec_block3(dec_block3)\n",
    "        \n",
    "        dec_block4 = self.upsample_and_concat(dec_block3, enc_block1) # voeg bottleneck en enc_block4\n",
    "        dec_block4 = self.dec_block4(dec_block4)\n",
    "\n",
    "        output = torch.sigmoid(self.final_conv(dec_block4)) # sigmoid want binary classification\n",
    "        return output\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, stride=1, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=1, kernel_size=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def upsample_and_concat(self, x1, x2):\n",
    "        x1 = nn.functional.interpolate(x1, scale_factor=2, mode='bilinear') # upsample (grone pijl in de slides)\n",
    "        x2 = torchvision.transforms.functional.resize(x2, x1.shape[2]) # de grijze pijl in de slides\n",
    "        return torch.cat([x1, x2], dim=1) # hange het witte blokje aan het blauwe blokje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd905be4-8eaa-4fd3-acca-e53a30e5c4f4",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4d75f-9420-49fe-ae02-78f1dd46c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs=2\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for idx, (images, masks) in enumerate(train_loader):\n",
    "        images, masks = images.to(device), masks.to(device) # stuur het naar de gpu als je er 1 hebt\n",
    "        \n",
    "        #optimizer.zero_grad()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        outputs = model(images)\n",
    "        #print(outputs.shape)\n",
    "        #print(outputs.max())\n",
    "        #break\n",
    "        loss = criterion(outputs, masks.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx%100==99:\n",
    "            print(f\"Busy with batch {idx}/{len(train_loader)}\")\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch}: running loss is {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e85619b-045a-4bb0-8f90-c7cb5ffadf6f",
   "metadata": {},
   "source": [
    "## Evaluatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6daad-fc99-4122-b651-4eca002954b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalueren en Visualiseren\n",
    "model.eval()\n",
    "\n",
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "    \n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(display_list[i].cpu().numpy().squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Testen op enkele voorbeelden\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        predictions = model(images)\n",
    "        print(masks[0].unique())\n",
    "        predictions = (predictions > 0.5).float()  # Thresholding\n",
    "        display([images[0].permute(1, 2, 0), masks[0], predictions[0]])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b5324-b144-4b89-b474-e1eff0c49f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kleiner model om te testen\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = OxfordPetsDataset(root=root, transform=transform, output_size=128)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "encoder = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, stride=1, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(16, 32, stride=1, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(32, 64, stride=1, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(64, 128, stride=1, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2)\n",
    ")\n",
    "\n",
    "decoder=nn.Sequential(\n",
    "    nn.ConvTranspose2d(128, 64, stride=1, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "    nn.ConvTranspose2d(64, 32, stride=1, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "    nn.ConvTranspose2d(32, 16, stride=1, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "    nn.ConvTranspose2d(16, 1, stride=1, kernel_size=3, padding=1),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest')\n",
    ")\n",
    "\n",
    "small_model = nn.Sequential(encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd815789-c063-4d43-a24c-78a1efb5fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs=2\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for idx, (images, masks) in enumerate(train_loader):\n",
    "        #optimizer.zero_grad()\n",
    "        small_model.zero_grad(set_to_none=True)\n",
    "        outputs = small_model(images)\n",
    "        loss = criterion(outputs, masks.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx%100==99:\n",
    "            print(f\"Busy with batch {idx}/{len(train_loader)}\")\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch}: running loss is {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cbdf6c-42fc-47d9-b41d-a0309c186ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "    \n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(display_list[i].cpu().numpy().squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Testen op enkele voorbeelden\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        predictions = small_model(images)\n",
    "        print(masks[0].unique())\n",
    "        predictions = (predictions > 0.5).float()*255  # Thresholding\n",
    "        display([images[0].permute(1, 2, 0), masks[0], predictions[0]])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd0b77-e172-412c-b631-065ea2ec3ea2",
   "metadata": {},
   "source": [
    "## Oefening\n",
    "\n",
    "Herbouw het bovenstaande model met Keras, zorg er ook voor dat de loss bestudeerd kan worden met tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594e06dc-719c-4588-b10b-f7cd7153a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from keras.utils import Sequence, to_categorical\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669fb4a2-3602-4548-9d76-e4fc038932f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84064d60-630f-4fd6-a84b-29f800f1b6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5618eb5-610d-4027-93d4-a65cc53679e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
