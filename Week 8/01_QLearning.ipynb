{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iCCMpkHqd_R"
   },
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "Reinforcement learning komt uit de studie van Markov Chains of Processen voor.\n",
    "Dit is een random opeenvolging van states waarbij elke transisitie een mogelijke kans heeft.\n",
    "Door een reward te koppelen aan elke state waarin je komt kan je een functie opstellen die de de totale reward maximaliseert.\n",
    "Dit is het basisidee achter reinforcement learning.\n",
    "\n",
    "Een aantal belangrijke termen/concepten hierbij zijn:\n",
    "* De agent\n",
    "* Het environment\n",
    "* De state space\n",
    "* De action space\n",
    "* De reward en return\n",
    "* Exploration vs exploitation\n",
    "\n",
    "## Q-learning\n",
    "\n",
    "Een eerste algoritme dat we bekijken voor reinforcement learning uit te voeren is Q-learning.\n",
    "Dit algoritme maakt gebruik van de Q-functie of action-value function.\n",
    "Hiervoor houdt het Q-learning algoritme een matrix bij dat de reward van actie in een state bepaald.\n",
    "In een verkenningsfase laten we toe dat er sub-optimale keuzes genomen worden.\n",
    "Nadat dit lang genoeg gerund heeft, gaan we over naar een exploitation fase waarbij enkel de beste keuzes genomen worden.\n",
    "\n",
    "Om te tonen hoe je het Q-learning algoritme kan implementeren, kan je gebruik maken van het gymnasium package.\n",
    "Dit bevat heel wat eenvoudige environments van spelletjes in python die hiervoor gebruikt kunnen worden.\n",
    "De bron voor onderstaande code komt van een [tutorial van de library](https://gymnasium.farama.org/tutorials/training_agents/blackjack_tutorial/)\n",
    "In deze code gaan we experimenteren met het blackjack environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create the Blackjack environment\n",
    "env = gym.make('Blackjack-v1')\n",
    "\n",
    "# Start a new Blackjack game (reset environment)\n",
    "state = env.reset()\n",
    "\n",
    "# Define actions (0 = Stick, 1 = Hit)\n",
    "actions = {0: 'Stick', 1: 'Hit'}\n",
    "\n",
    "# Function to display the state\n",
    "def display_state(state):\n",
    "    player_hand, dealer_hand, usable_ace = state\n",
    "    print(f\"Player's hand: {player_hand}, Dealer's showing: {dealer_hand}, Usable ace: {usable_ace}\")\n",
    "\n",
    "# Display initial state\n",
    "display_state(state[0])\n",
    "\n",
    "# Interactive function to play Blackjack manually\n",
    "def play_blackjack(env):\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = int(input(\"Choose action - 0 (Stick) or 1 (Hit): \"))\n",
    "        \n",
    "        # Take the action\n",
    "        state, reward, terminated, trunctated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Display the state after the action\n",
    "        display_state(state)\n",
    "        \n",
    "        # Check if the game is finished\n",
    "        if done:\n",
    "            if reward > 0:\n",
    "                print(f\"You won! Reward: {reward}\")\n",
    "            elif reward < 0:\n",
    "                print(f\"You lost! Reward: {reward}\")\n",
    "            else:\n",
    "                print(f\"Game ended in a draw. Reward: {reward}\")\n",
    "\n",
    "# Start the game\n",
    "play_blackjack(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merk op dat er twee mogelijke acties zijn: 0 en 1 om een extra kaart te nemen of niet.\n",
    "\n",
    "De state bestaat uit drie waarden:\n",
    "* Je eigen hand\n",
    "* De hand van de dealer\n",
    "* Het aantal bruikbare aces\n",
    "\n",
    "In de code hieronder gaan we een agent schrijven die het Q-Learning algoritme implementeert voor het blackjack environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BlackjackAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values = {}   # Gaan we de Q-values in opslaan (1 per observatie)\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        if obs not in self.q_values:\n",
    "            self.q_values[obs] = np.zeros(env.action_space.n)\n",
    "            \n",
    "        if np.random.random() < self.epsilon:\n",
    "            # exploration (random)\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            # exploitation (greedy)\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        if obs not in self.q_values:\n",
    "            self.q_values[obs] = np.zeros(env.action_space.n)\n",
    "\n",
    "        if next_obs not in self.q_values:\n",
    "            self.q_values[next_obs] = np.zeros(env.action_space.n)\n",
    "\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        # Update the Q-value\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):            # in het begin grote epsilon -> neem veel random zetten, later kleinere epsilon zodat er vooral de beste zet genomen wordt\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_episodes = 100_000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BlackjackAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Op dit moment kunnen we deze agent een aantal keer blackjack laten spelen om te leren welke acties tot positieve rewards en welke tot negatieve rewards leiden.\n",
    "Dit gebeurd in de code hieronder waar we ook gebruik maken van de tqdm package om een progress-bar te tonen tijdens het uitvoeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the Blackjack environment\n",
    "env = gym.make('Blackjack-v1')\n",
    "\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update the agent\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bovenstaande leerproces is echter nog niet geevalueerd. We hebben dus nog geen idee of de geleerde agent een gewenst gedrag geleerd heeft.\n",
    "Om deze reden is er hierboven gebruik gemaakt van de RecordEpisodeStatistics wrapper die informatie bijhoudt over het leerproces.\n",
    "Hieronder maken we enkele grafieken om de prestatie van de agent te analyseren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rolling_length = 500\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "axs[0].set_title(\"Episode rewards\")\n",
    "# compute and assign a rolling average of the data to provide a smoother graph\n",
    "reward_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "axs[1].set_title(\"Episode lengths\")\n",
    "length_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "axs[2].set_title(\"Training Error\")\n",
    "training_error_moving_average = (\n",
    "    np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode=\"same\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def create_grids(agent, usable_ace=False):\n",
    "    \"\"\"Create value and policy grid given an agent.\"\"\"\n",
    "    # convert our state-action values to state values\n",
    "    # and build a policy dictionary that maps observations to actions\n",
    "    state_value = defaultdict(float)\n",
    "    policy = defaultdict(int)\n",
    "    for obs, action_values in agent.q_values.items():\n",
    "        state_value[obs] = float(np.max(action_values))\n",
    "        policy[obs] = int(np.argmax(action_values))\n",
    "\n",
    "    player_count, dealer_count = np.meshgrid(\n",
    "        # players count, dealers face-up card\n",
    "        np.arange(12, 22),\n",
    "        np.arange(1, 11),\n",
    "    )\n",
    "\n",
    "    # create the value grid for plotting\n",
    "    value = np.apply_along_axis(\n",
    "        lambda obs: state_value[(obs[0], obs[1], usable_ace)],\n",
    "        axis=2,\n",
    "        arr=np.dstack([player_count, dealer_count]),\n",
    "    )\n",
    "    value_grid = player_count, dealer_count, value\n",
    "\n",
    "    # create the policy grid for plotting\n",
    "    policy_grid = np.apply_along_axis(\n",
    "        lambda obs: policy[(obs[0], obs[1], usable_ace)],\n",
    "        axis=2,\n",
    "        arr=np.dstack([player_count, dealer_count]),\n",
    "    )\n",
    "    return value_grid, policy_grid\n",
    "\n",
    "\n",
    "def create_plots(value_grid, policy_grid, title: str):\n",
    "    \"\"\"Creates a plot using a value and policy grid.\"\"\"\n",
    "    # create a new figure with 2 subplots (left: state values, right: policy)\n",
    "    player_count, dealer_count, value = value_grid\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.4))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # plot the state values\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "    ax1.plot_surface(\n",
    "        player_count,\n",
    "        dealer_count,\n",
    "        value,\n",
    "        rstride=1,\n",
    "        cstride=1,\n",
    "        cmap=\"viridis\",\n",
    "        edgecolor=\"none\",\n",
    "    )\n",
    "    plt.xticks(range(12, 22), range(12, 22))\n",
    "    plt.yticks(range(1, 11), [\"A\"] + list(range(2, 11)))\n",
    "    ax1.set_title(f\"State values: {title}\")\n",
    "    ax1.set_xlabel(\"Player sum\")\n",
    "    ax1.set_ylabel(\"Dealer showing\")\n",
    "    ax1.zaxis.set_rotate_label(False)\n",
    "    ax1.set_zlabel(\"Value\", fontsize=14, rotation=90)\n",
    "    ax1.view_init(20, 220)\n",
    "\n",
    "    # plot the policy\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap=\"Accent_r\", cbar=False)\n",
    "    ax2.set_title(f\"Policy: {title}\")\n",
    "    ax2.set_xlabel(\"Player sum\")\n",
    "    ax2.set_ylabel(\"Dealer showing\")\n",
    "    ax2.set_xticklabels(range(12, 22))\n",
    "    ax2.set_yticklabels([\"A\"] + list(range(2, 11)), fontsize=12)\n",
    "\n",
    "    # add a legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=\"lightgreen\", edgecolor=\"black\", label=\"Hit\"),\n",
    "        Patch(facecolor=\"grey\", edgecolor=\"black\", label=\"Stick\"),\n",
    "    ]\n",
    "    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))\n",
    "    return fig\n",
    "\n",
    "\n",
    "# state values & policy with usable ace (ace counts as 11)\n",
    "value_grid, policy_grid = create_grids(agent, usable_ace=True)\n",
    "fig1 = create_plots(value_grid, policy_grid, title=\"With usable ace\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state values & policy without usable ace (ace counts as 1)\n",
    "value_grid, policy_grid = create_grids(agent, usable_ace=False)\n",
    "fig2 = create_plots(value_grid, policy_grid, title=\"Without usable ace\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoe goed presteert de agent?\n",
    "\n",
    "In onderstaande code gaan we de agent een 10-tal keer laten blackjack spelen. \n",
    "Hierbij gaan we bijhouden hoe vaak de agent wint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Blackjack environment\n",
    "env = gym.make('Blackjack-v1')\n",
    "\n",
    "# Define actions (0 = Stick, 1 = Hit)\n",
    "actions = {0: 'Stick', 1: 'Hit'}\n",
    "\n",
    "# Function to display the state\n",
    "def display_state(state):\n",
    "    player_hand, dealer_hand, usable_ace = state\n",
    "    print(f\"Player's hand: {player_hand}, Dealer's showing: {dealer_hand}, Usable ace: {usable_ace}\")\n",
    "\n",
    "# Function to play one game of Blackjack\n",
    "def play_blackjack(env, agent=None, random=False):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        if agent is None and random:\n",
    "            action = random.randint(0, 1)\n",
    "        elif agent is None:\n",
    "            action = int(input(\"Choose action - 0 (Stick) or 1 (Hit): \"))\n",
    "        else:\n",
    "            action = agent.get_action(obs)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Optionally, display the state after the action\n",
    "        display_state(state)\n",
    "        \n",
    "    return reward\n",
    "\n",
    "# Number of games to play\n",
    "num_games = 10\n",
    "wins = 0\n",
    "\n",
    "# Play the game num_games times\n",
    "for i in range(num_games):\n",
    "    reward = play_blackjack(env, agent)\n",
    "    if reward > 0:\n",
    "        wins += 1\n",
    "    elif reward < 0:\n",
    "        print(f\"Game {i+1}: You lost! Reward: {reward}\")\n",
    "    else:\n",
    "        print(f\"Game {i+1}: Game ended in a draw. Reward: {reward}\")\n",
    "\n",
    "print(f\"Total games played: {num_games}\")\n",
    "print(f\"Number of wins: {wins}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL in neural networks\n",
    "\n",
    "Het gebruik van Q-learning werkt goed als het aantal states en acties beperkt zijn.\n",
    "Dit is echter zelden het geval, denk bijvoorbeeld aan een continue variabele zoals snelheid of locatie.\n",
    "\n",
    "Een oplossing hiervoor is om de action-value functie die in Q-learning geoptimaliseerd wordt te benaderen ipv exact te berekenen.\n",
    "Dit kan bijvoorbeeld door middel van een neuraal netwerk te gebruiken.\n",
    "Er zijn verschillende model-structuren die hiervoor ontwikkeld zijn zoals:\n",
    "- DQN (onderwerp van onderstaande demo)\n",
    "- REINFORCE\n",
    "- DDPG\n",
    "- TD3\n",
    "- PPO\n",
    "- SAC\n",
    "\n",
    "Voor we beginnen met het uitwerken van een model.\n",
    "Bekijk [deze tutorial](https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial) en beantwoord de volgende vragen:\n",
    "- Wat is de state en wat zijn de mogelijke acties?\n",
    "- Wat is de structuur van het gebruikte DQN?\n",
    "- Zijn er nieuwe hyperparameters gebruikt?\n",
    "- Welke metriek wordt er gebruikt en waar wordt deze berekend?\n",
    "- Hoe worden de gewichten aangepast?\n",
    "- Waarvoor wordt de ReplayBuffer gebruikt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Antwoord:**\n",
    "- Vraag 1:\n",
    "    - State: de positie en snelheid van het karretje en de hoek/hoeksnelheid van de staaf.\n",
    "    - Acties: Beweeg naar links en beweeg naar rechts\n",
    "- Vraag 2: Er zijn drie lagen met respectievelijk 100, 50 en 2 neuronen. Het is belangrijk dat het aantal neuronen in de laatste laag overeenkomt met het aantal acties.\n",
    "- Vraag 3: De enige nieuwe hyperparameter bij het aanmaken van het neuraal netwerk is de initialiser. De hidden lagen gebruiken een VarianceScaler als kernel-initalisator wat inhoudt dat ze gesampled worden uit een Normaalverdeling. De outputlayer gebruikt een RandomUniform kernel-initializer (sample de gewichten uit een uniforme verdeling) en een constante waarde als bias-initializer\n",
    "- Vraag 4: De average return wordt hiervoor gebruikt en deze wordt [hier](https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial#metrics_and_evaluation) berekend. De return is de tijd dat de staaf omhoog blijft (1 voor elke tijdstap)\n",
    "- Vraag 5 en 6: Je laadt het netwerk wat lopen, de uitgevoerde acties en bekomen rewards worden opgeslaan in de ReplayBuffer. Batches of data worden uit de replaybuffer gehaald om het netwerk te trainen op basis van de gemiddelde return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf nu zelf de nodige code om het DQN-model toe te passen op het \"Pole-cart\" environment van gymnasium.\n",
    "Hieronder vind je de nodige code om een visualisatie te maken van het uitvoeren van het pole-cart environment.\n",
    "Om ervoor te zorgen dat de simulatie niet te snel stopt worden er altijd 1000 stappen gesimuleerd en wordt het gereset bij falen.\n",
    "In de realiteit stop je het environment na een kritisch falen echter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def display_env(env):\n",
    "    \"\"\"Function to display the environment inline in a Jupyter notebook.\"\"\"\n",
    "    img = env.render()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    display(plt.gcf())\n",
    "    clear_output(wait=True)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(100):\n",
    "\n",
    "    display_env(env)\n",
    "    \n",
    "    action = env.action_space.sample()  # this is where you would insert your policy\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a neural network to approximate the Q-value function (regression for each possible action).\n",
    "De bron van deze code kan je hier vinden: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython.display import display, clear_output\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om een DQN-netwerk stabieler te trainen wordt een replay-buffer in het leven geroepen.\n",
    "Deze houdt alle observaties bij die tegenkomen worden en maakt het mogelijk om hier samples uit te nemen.\n",
    "Dit zorgt ervoor dat de tijds-correlatie verdwijnt wat het resultaat stabieler maakt.\n",
    "Deze buffer schrijven we als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# welke data bijgehouden wordt in het geheugen\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daarna kunnen we het neuraal netwerk opbouwen om de Q-value horende bij de acties te benaderen voor elke state.\n",
    "Het aantal inputs is dus gelijk aan de observaties en de outputs aan het aantal mogelijke acties.\n",
    "De opbouw van het netwerk ertussen kan je vrij kiezen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:25\u001b[0;36m\u001b[0m\n\u001b[0;31m    def forward(self, state):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "     def __init__(self, state_size, action_size, layer_sizes=(100, 50), learning_rate=0.0001):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.layer_sizes = layer_sizes\n",
    "\n",
    "        # Define the network architecture\n",
    "        layers = []\n",
    "        input_size = state_size\n",
    "\n",
    "        for hidden_size in layer_sizes:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = hidden_size\n",
    "\n",
    "        layers.append(nn.Linear(input_size, action_size))  # Output layer with action_size units\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        # Define the optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.network(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Op basis van dit netwerk kunnenn we dan een agent definieren die het mogelijk maakt om het netwerk te trainen en acties te voorspellen.\n",
    "De basis-structuur van deze agent ziet eruit als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, env, buffer_size=int(1e5), batch_size=64, gamma=0.99, tau=1e-3, lr=5e-4, eps_start=0.9, eps_end=0.05, eps_decay=1000, layers=[20]):\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        self.steps = 0\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.Q_targets = 0.0\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "\n",
    "        print(\"Initialising DDQN Agent with params : {}\".format(self.__dict__))\n",
    "\n",
    "        # Make local & target model\n",
    "        print(\"Initialising Local DQNetwork\")\n",
    "        self.policy_network = DQNetwork(self.state_size, self.action_size,\n",
    "                                       layer_sizes=layers,\n",
    "                                       learning_rate=lr)\n",
    "\n",
    "        print(\"Initialising Target DQNetwork\")\n",
    "        self.target_network = DQNetwork(self.state_size, self.action_size,\n",
    "                                        layer_sizes=layers,\n",
    "                                        learning_rate=lr)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "        self.memory = ReplayMemory(capacity=buffer_size)\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_state = state\n",
    "        self.steps = 0\n",
    "        return state\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1. * self.steps / self.eps_decay)\n",
    "        self.steps += 1\n",
    "        \n",
    "        if random.random() > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_network(state).max(1).indices.item()\n",
    "        else:\n",
    "            return np.random.randint(self.action_size)\n",
    "\n",
    "    def step(self, state, action, next_state, reward, done):\n",
    "        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "        action = torch.tensor(action, dtype=torch.float32).unsqueeze(0)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "            \n",
    "        self.memory.push(state, action, next_state, reward)\n",
    "        self.optimize_model()\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))  \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action).to(torch.int64).unsqueeze(-1)\n",
    "        reward_batch = torch.cat(batch.reward)   \n",
    "        \n",
    "        # Q(s_t, a)\n",
    "        state_action_values = self.policy_network(state_batch).gather(1, action_batch) \n",
    "        \n",
    "        next_state_values = torch.zeros(self.batch_size)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1).values\n",
    "        \n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # update weights\n",
    "        loss = self.policy_network.criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.policy_network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_network.parameters(), 100) # In-place gradient clipping\n",
    "        self.policy_network.optimizer.step()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        target_net_state_dict = self.target_network.state_dict()\n",
    "        policy_net_state_dict = self.policy_network.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1-self.tau)\n",
    "        self.target_network.load_state_dict(target_net_state_dict)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display(plt.gcf())\n",
    "            clear_output(wait=True)\n",
    "        else:\n",
    "            display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 304/500 [12:48<27:38,  8.46s/it]"
     ]
    }
   ],
   "source": [
    "#training the agent\n",
    "from tqdm import tqdm\n",
    "from itertools import count\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "agent = DQNAgent(env, \n",
    "                 buffer_size=10000, \n",
    "                 batch_size=16, \n",
    "                 gamma=0.99, \n",
    "                 tau=1e-3, \n",
    "                 lr=5e-4, \n",
    "                 eps_start=0.9, eps_end=0.05, eps_decay=1000, \n",
    "                 layers=[20])\n",
    "num_episodes = 500\n",
    "\n",
    "for i_episode in tqdm(range(num_episodes)):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    agent.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        agent.step(state, action, next_state, reward, done)\n",
    "        \n",
    "    episode_durations.append(agent.steps + 1)\n",
    "    plot_durations()\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Function to play one game of Blackjack\n",
    "def play(env, agent):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        display_env(env)\n",
    "        \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(env,agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
