{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e730cc-69c0-4b82-bb58-25a163fa4aad",
   "metadata": {},
   "source": [
    "# Variational Autoencoder met Fashion MNIST\n",
    "\n",
    "Een Variational Autoencoder (VAE) is een type autoencoder dat wordt gebruikt voor het genereren van nieuwe voorbeelden die vergelijkbaar zijn met de dataset waarop het is getraind. \n",
    "Het bestaat uit een encoder die de input data naar een lagere-dimensionale latente ruimte projecteert, en een decoder die uit deze latente ruimte nieuwe data reconstrueert.\n",
    "\n",
    "Overzicht:\n",
    "* Importeren van bibliotheken en dataset\n",
    "* Definiëren van de VAE-architectuur\n",
    "* Trainen van het VAE-model\n",
    "* Genereren van nieuwe afbeeldingen met de VAE\n",
    "## Importeren van packages en dataset\n",
    "\n",
    "Eerst importeren we alle benodigde Python-bibliotheken voor het bouwen, trainen en visualiseren van onze VAE.\n",
    "We gebruiken Pytorch voor het bouwen van het neurale netwerk, matplotlib voor visualisaties en NumPy voor numerieke berekeningen.\n",
    "Daarna laden we de Fashion MNIST dataset, normaliseren de pixelwaarden naar de range [0,1] \n",
    "en splitsen de dataset in een trainings- en testset. We gebruiken DataLoader om mini-batches te maken voor training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737c9cf-73df-433c-95b1-aa0c9e36f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check of GPU beschikbaar is\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Apparaat gebruikt voor training: {device}\")\n",
    "\n",
    "# Definieer transformatie (normaliseren van beelden)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converteer afbeeldingen naar PyTorch tensors\n",
    "])\n",
    "\n",
    "# Laad de Fashion MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Creëer DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Aantal trainingsbatches: {len(train_loader)}, Aantal testbatches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62ec61-8ac0-42a1-ba86-e73bafdc6503",
   "metadata": {},
   "source": [
    "## Definiëren van de VAE-architectuur\n",
    "\n",
    "De architectuur van onze Variational Autoencoder bestaat uit een encoder die de inputbeelden naar een lagere-dimensionale latente ruimte projecteert en een decoder die deze latente ruimte gebruikt om de afbeeldingen opnieuw te genereren.\n",
    "De verliesfunctie voor onze VAE, bestaande uit de som van de reconstructieverlies (binary cross-entropy)\n",
    "en de KL-divergentie. We gebruiken de Adam optimizer voor het bijwerken van de gewichten van het netwerk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3be7c3-68ec-4c0b-8491-959e28e5138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # Output: [32, 14, 14]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Output: [64, 7, 7]\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()  # Output: [64*7*7]\n",
    "        )\n",
    "        \n",
    "        # Linear layers voor mu en logvar\n",
    "        self.fc_mu = nn.Linear(64*7*7, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64*7*7, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, 64*7*7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: [32, 14, 14]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: [1, 28, 28]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 64, 7, 7)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Instantieer het model\n",
    "latent_dim = 2\n",
    "model = VariationalAutoencoder(latent_dim).to(device)\n",
    "print(model)\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    # Reconstructie verlies (binary cross-entropy)\n",
    "    recon_loss = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL-divergentie verlies\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Totale VAE verlies\n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "# Definieer de optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6e15e2-f0ab-4cb9-9131-9cdb5295c785",
   "metadata": {},
   "source": [
    "## Trainen van het VAE model\n",
    "\n",
    "In deze cel trainen we het VAE-model met de trainingsgegevens. \n",
    "Voor elke epoch voeren we een forward pass, berekenen we het verlies, en voeren we een backward pass uit om de gewichten bij te werken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b39ed-f6fd-41de-b26b-8b76eba1b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aantal epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_images, mu, logvar = model(images)\n",
    "        \n",
    "        # Bereken het verlies met de aangepaste BCE en KL-divergentie\n",
    "        loss = vae_loss(recon_images, images, mu, logvar)\n",
    "        \n",
    "        # Backward pass en optimalisatie\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Gemiddelde verlies voor de huidige epoch\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a911668-67ab-4f76-bd89-99f318b6fb6d",
   "metadata": {},
   "source": [
    "## Genereren van nieuwe resultaten\n",
    "\n",
    "Eerst bestuderen we de latente ruimte door de testset in kaart te brengen in de 2D latente ruimte van de VAE. Hiermee kunnen we zien hoe goed de VAE leert om vergelijkbare afbeeldingen bij elkaar te plaatsen.\n",
    "Daarna genereren we nieuwe afbeeldingen door willekeurige punten te nemen uit de latente ruimte \n",
    "en deze te decoderen met behulp van de decoder van de VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aea7ba7-f411-45df-98f8-90d645ee616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "z_points = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        mu, _ = model.encode(images)\n",
    "        z_points.append(mu.cpu().numpy())\n",
    "\n",
    "# Zet alle latente punten in een array\n",
    "z_points = np.concatenate(z_points)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(z_points[:, 0], z_points[:, 1], c='blue', alpha=0.5)\n",
    "plt.xlabel(\"z[0]\")\n",
    "plt.ylabel(\"z[1]\")\n",
    "plt.title(\"Visualisatie van de latente ruimte\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf185db7-75c2-440f-87d8-36de4adaad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereer nieuwe afbeeldingen van random punten in de latente ruimte\n",
    "n = 10  # Aantal afbeeldingen per rij/kolom\n",
    "figure = np.zeros((28 * n, 28 * n))\n",
    "\n",
    "# Uniform verdeeld random punten in de latente ruimte\n",
    "grid_x = np.linspace(-3, 3, n)\n",
    "grid_y = np.linspace(-3, 3, n)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = torch.tensor([[xi, yi]], dtype=torch.float32).to(device)\n",
    "            x_decoded = model.decode(z_sample).cpu().numpy()\n",
    "            digit = x_decoded[0].reshape(28, 28)\n",
    "            figure[i * 28: (i + 1) * 28, j * 28: (j + 1) * 28] = digit\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.title(\"Gegenereerde afbeeldingen van random punten in de latente ruimte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189d0db-f274-4810-abb3-5f3d5aa091ed",
   "metadata": {},
   "source": [
    "# Oefening: denoising autoencoder\n",
    "\n",
    "Een denoising autoencoder (DAE) is een type autoencoder dat is ontworpen om ruis uit de invoergegevens te verwijderen. Het is een populaire techniek in deep learning voor het voorverwerken van gegevens, het leren van robuuste representaties, en voor compressiedoeleinden.\n",
    "\n",
    "In deze oefening ga je een denoising autoencoder implementeren in PyTorch. Deze oefening omvat de volgende stappen:\n",
    "\n",
    "* Data voorbereiden: We gebruiken de Fashion MNIST dataset, voegen ruis toe aan de afbeeldingen en schalen de gegevens naar het bereik [0, 1]. (Dit deel krijg je hieronder)\n",
    "* Model bouwen: We bouwen een eenvoudig denoising autoencoder model met een encoder en een decoder.\n",
    "* Model trainen: We trainen het model met ruisachtige afbeeldingen als invoer en niet-vervuilde afbeeldingen als doel.\n",
    "* Resultaten evalueren: We testen het model door enkele ruisachtige afbeeldingen door het netwerk te laten gaan en hun gereconstrueerde versies weer te geven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c51aee-5ebb-466c-9a9c-cdecdffa410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data voorbereiden\n",
    "\n",
    "# Importeren van benodigde bibliotheken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Controleer of er een GPU beschikbaar is, zo niet gebruik de CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data-transformatie: normaliseer de afbeeldingen zodat de pixelwaarden tussen 0 en 1 liggen\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Converteert beeld naar tensor en schaalt automatisch naar [0, 1]\n",
    "])\n",
    "\n",
    "# FashionMNIST dataset downloaden en laden\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# DataLoader voor batches van de trainings- en testdata\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Data geladen en DataLoader klaar.\")\n",
    "\n",
    "# Functie om ruis aan de afbeeldingen toe te voegen\n",
    "def add_noise(imgs, noise_factor=0.5):\n",
    "    noisy_imgs = imgs + noise_factor * torch.randn_like(imgs)\n",
    "    noisy_imgs = torch.clamp(noisy_imgs, 0., 1.)\n",
    "    return noisy_imgs\n",
    "\n",
    "# Visualisatie van enkele ruisachtige afbeeldingen\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, _) = next(examples)\n",
    "\n",
    "noisy_imgs = add_noise(example_data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 4))\n",
    "for i in range(5):\n",
    "    axes[i].imshow(noisy_imgs[i].squeeze().numpy(), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235363bd-2559-49a3-8a2a-6045643b1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model bouwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c5b8f-c716-4feb-aa5d-98047c2e213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model trainen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ce60f-3f5b-4e90-8d58-2710efbd2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultaten visualiseren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ce740-c79d-4630-bc16-53dc730226e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
