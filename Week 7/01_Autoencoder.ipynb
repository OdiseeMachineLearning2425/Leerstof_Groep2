{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e730cc-69c0-4b82-bb58-25a163fa4aad",
   "metadata": {},
   "source": [
    "# Variational Autoencoder met Fashion MNIST\n",
    "\n",
    "Een Variational Autoencoder (VAE) is een type autoencoder dat wordt gebruikt voor het genereren van nieuwe voorbeelden die vergelijkbaar zijn met de dataset waarop het is getraind. \n",
    "Het bestaat uit een encoder die de input data naar een lagere-dimensionale latente ruimte projecteert, en een decoder die uit deze latente ruimte nieuwe data reconstrueert.\n",
    "\n",
    "Overzicht:\n",
    "* Importeren van bibliotheken en dataset\n",
    "* Definiëren van de VAE-architectuur\n",
    "* Trainen van het VAE-model\n",
    "* Genereren van nieuwe afbeeldingen met de VAE\n",
    "## Importeren van packages en dataset\n",
    "\n",
    "Eerst importeren we alle benodigde Python-bibliotheken voor het bouwen, trainen en visualiseren van onze VAE.\n",
    "We gebruiken Pytorch voor het bouwen van het neurale netwerk, matplotlib voor visualisaties en NumPy voor numerieke berekeningen.\n",
    "Daarna laden we de Fashion MNIST dataset, normaliseren de pixelwaarden naar de range [0,1] \n",
    "en splitsen de dataset in een trainings- en testset. We gebruiken DataLoader om mini-batches te maken voor training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1737c9cf-73df-433c-95b1-aa0c9e36f383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apparaat gebruikt voor training: cpu\n",
      "Aantal trainingsbatches: 469, Aantal testbatches: 79\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check of GPU beschikbaar is\n",
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Apparaat gebruikt voor training: {device}\")\n",
    "\n",
    "# Definieer transformatie (normaliseren van beelden)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converteer afbeeldingen naar PyTorch tensors\n",
    "])\n",
    "\n",
    "# Laad de Fashion MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Creëer DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Aantal trainingsbatches: {len(train_loader)}, Aantal testbatches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62ec61-8ac0-42a1-ba86-e73bafdc6503",
   "metadata": {},
   "source": [
    "## Definiëren van de VAE-architectuur\n",
    "\n",
    "De architectuur van onze Variational Autoencoder bestaat uit een encoder die de inputbeelden naar een lagere-dimensionale latente ruimte projecteert en een decoder die deze latente ruimte gebruikt om de afbeeldingen opnieuw te genereren.\n",
    "De verliesfunctie voor onze VAE, bestaande uit de som van de reconstructieverlies (binary cross-entropy)\n",
    "en de KL-divergentie. We gebruiken de Adam optimizer voor het bijwerken van de gewichten van het netwerk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3be7c3-68ec-4c0b-8491-959e28e5138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=3136, out_features=2, bias=True)\n",
      "  (fc_logvar): Linear(in_features=3136, out_features=2, bias=True)\n",
      "  (decoder_input): Linear(in_features=2, out_features=3136, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # Output: [32, 14, 14]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Output: [64, 7, 7]\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()  # Output: [64*7*7]\n",
    "        )\n",
    "        \n",
    "        # Linear layers voor mu en logvar\n",
    "        self.fc_mu = nn.Linear(64*7*7, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64*7*7, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, 64*7*7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: [32, 14, 14]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: [1, 28, 28]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 64, 7, 7)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Instantieer het model\n",
    "latent_dim = 2\n",
    "model = VariationalAutoencoder(latent_dim).to(device)\n",
    "print(model)\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    # Reconstructie verlies (binary cross-entropy)\n",
    "    recon_loss = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL-divergentie verlies\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Totale VAE verlies\n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "# Definieer de optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6e15e2-f0ab-4cb9-9131-9cdb5295c785",
   "metadata": {},
   "source": [
    "## Trainen van het VAE model\n",
    "\n",
    "In deze cel trainen we het VAE-model met de trainingsgegevens. \n",
    "Voor elke epoch voeren we een forward pass, berekenen we het verlies, en voeren we een backward pass uit om de gewichten bij te werken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b39ed-f6fd-41de-b26b-8b76eba1b845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading existing model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set model to evaluation mode if only inference is required\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Aantal epochs\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1443\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1382\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1383\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1384\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1387\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:118\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m     )\n\u001b[0;32m--> 118\u001b[0m     \u001b[43muntyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the path to save/load the model\n",
    "model_path = \"01_vaemodel.pth\"\n",
    "\n",
    "# Check if the model file exists\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading existing model...\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # Set model to evaluation mode if only inference is required\n",
    "else:\n",
    "    # Aantal epochs\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            recon_images, mu, logvar = model(images)\n",
    "            \n",
    "            # Bereken het verlies met de aangepaste BCE en KL-divergentie\n",
    "            loss = vae_loss(recon_images, images, mu, logvar)\n",
    "            \n",
    "            # Backward pass en optimalisatie\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Gemiddelde verlies voor de huidige epoch\n",
    "        avg_loss = train_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save the model after training\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a911668-67ab-4f76-bd89-99f318b6fb6d",
   "metadata": {},
   "source": [
    "## Genereren van nieuwe resultaten\n",
    "\n",
    "Eerst bestuderen we de latente ruimte door de testset in kaart te brengen in de 2D latente ruimte van de VAE. Hiermee kunnen we zien hoe goed de VAE leert om vergelijkbare afbeeldingen bij elkaar te plaatsen.\n",
    "Daarna genereren we nieuwe afbeeldingen door willekeurige punten te nemen uit de latente ruimte \n",
    "en deze te decoderen met behulp van de decoder van de VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aea7ba7-f411-45df-98f8-90d645ee616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "z_points = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        mu, _ = model.encode(images)\n",
    "        z_points.append(mu.cpu().numpy())\n",
    "\n",
    "# Zet alle latente punten in een array\n",
    "z_points = np.concatenate(z_points)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(z_points[:, 0], z_points[:, 1], c='blue', alpha=0.5)\n",
    "plt.xlabel(\"z[0]\")\n",
    "plt.ylabel(\"z[1]\")\n",
    "plt.title(\"Visualisatie van de latente ruimte\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf185db7-75c2-440f-87d8-36de4adaad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereer nieuwe afbeeldingen van random punten in de latente ruimte\n",
    "n = 10  # Aantal afbeeldingen per rij/kolom\n",
    "figure = np.zeros((28 * n, 28 * n))\n",
    "\n",
    "# Uniform verdeeld random punten in de latente ruimte\n",
    "grid_x = np.linspace(-3, 3, n)\n",
    "grid_y = np.linspace(-3, 3, n)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = torch.tensor([[xi, yi]], dtype=torch.float32).to(device)\n",
    "            x_decoded = model.decode(z_sample).cpu().numpy()\n",
    "            digit = x_decoded[0].reshape(28, 28)\n",
    "            figure[i * 28: (i + 1) * 28, j * 28: (j + 1) * 28] = digit\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.title(\"Gegenereerde afbeeldingen van random punten in de latente ruimte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189d0db-f274-4810-abb3-5f3d5aa091ed",
   "metadata": {},
   "source": [
    "# Oefening: denoising autoencoder\n",
    "\n",
    "Een denoising autoencoder (DAE) is een type autoencoder dat is ontworpen om ruis uit de invoergegevens te verwijderen. Het is een populaire techniek in deep learning voor het voorverwerken van gegevens, het leren van robuuste representaties, en voor compressiedoeleinden.\n",
    "\n",
    "In deze oefening ga je een denoising autoencoder implementeren in PyTorch. Deze oefening omvat de volgende stappen:\n",
    "\n",
    "* Data voorbereiden: We gebruiken de Fashion MNIST dataset, voegen ruis toe aan de afbeeldingen en schalen de gegevens naar het bereik [0, 1]. (Dit deel krijg je hieronder)\n",
    "* Model bouwen: We bouwen een eenvoudig denoising autoencoder model met een encoder en een decoder.\n",
    "* Model trainen: We trainen het model met ruisachtige afbeeldingen als invoer en niet-vervuilde afbeeldingen als doel.\n",
    "* Resultaten evalueren: We testen het model door enkele ruisachtige afbeeldingen door het netwerk te laten gaan en hun gereconstrueerde versies weer te geven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c51aee-5ebb-466c-9a9c-cdecdffa410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data voorbereiden\n",
    "\n",
    "# Importeren van benodigde bibliotheken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Controleer of er een GPU beschikbaar is, zo niet gebruik de CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data-transformatie: normaliseer de afbeeldingen zodat de pixelwaarden tussen 0 en 1 liggen\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Converteert beeld naar tensor en schaalt automatisch naar [0, 1]\n",
    "])\n",
    "\n",
    "# FashionMNIST dataset downloaden en laden\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# DataLoader voor batches van de trainings- en testdata\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Data geladen en DataLoader klaar.\")\n",
    "\n",
    "# Functie om ruis aan de afbeeldingen toe te voegen\n",
    "def add_noise(imgs, noise_factor=0.5):\n",
    "    noisy_imgs = imgs + noise_factor * torch.randn_like(imgs)\n",
    "    noisy_imgs = torch.clamp(noisy_imgs, 0., 1.)\n",
    "    return noisy_imgs\n",
    "\n",
    "# Visualisatie van enkele ruisachtige afbeeldingen\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, _) = next(examples)\n",
    "\n",
    "noisy_imgs = add_noise(example_data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 4))\n",
    "for i in range(5):\n",
    "    axes[i].imshow(noisy_imgs[i].squeeze().numpy(), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235363bd-2559-49a3-8a2a-6045643b1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model bouwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c5b8f-c716-4feb-aa5d-98047c2e213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model trainen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ce60f-3f5b-4e90-8d58-2710efbd2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultaten visualiseren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ce740-c79d-4630-bc16-53dc730226e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
